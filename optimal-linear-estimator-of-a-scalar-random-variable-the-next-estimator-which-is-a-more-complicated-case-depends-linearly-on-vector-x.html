<!DOCTYPE html>
    <html lang="en">
    
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-PQPWWZGD1B"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-PQPWWZGD1B');
        </script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7833742603914548"
     crossorigin="anonymous"></script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css" integrity="sha384-Xi8rHCmBmhbuyyhbI88391ZKP2dmfnOl4rT9ZfRI7mLTdk1wblIUnrIq35nqwEvC" crossorigin="anonymous">
        <link rel="stylesheet" href="../assets/css/min/@ui.css">
        <link rel="shortcut icon" href="../assets/images/favicons/favicon.ico">
        <link rel="apple-touch-icon-precomposed" href="../assets/images/favicons/apple-touch-icon-152x152-precomposed.png">
        <link rel="apple-touch-icon" sizes="180x180" href="../assets/images/favicons/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="../assets/images/favicons/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="../assets/images/favicons/favicon-16x16.png">
        <link rel="manifest" href="../assets/images/favicons/site.webmanifest">
        <link rel="mask-icon" href="../assets/images/favicons/safari-pinned-tab.svg" color="#10106d">
        <meta name="msapplication-TileColor" content="#10106d">
        <meta name="msapplication-config" content="/assets/images/favicons/browserconfig.xml">
        <meta name="theme-color" content="#10106d">
        <link rel="canonical" href="https://vlavar.com/engineeringdb/optimal-linear-estimator-of-a-scalar-random-variable-the-next-estimator-which-is-a-more-complicated-case-depends-linearly-on-vector-x" />
        <meta property="og:locale" content="en_US" />
        <meta property="og:type" content="article" />
        <meta property="og:site_name" content="Vlavar" />
        <meta property="article:publisher" content="admin@vlavar" />
       <title>Optimal Linear Estimator of a Scalar Random Variable The next estimator (which is a more complicated case) depends linearly on vector x: [solved] | Vlavar Engineering</title>
<meta property="og:title" content="Optimal Linear Estimator of a Scalar Random Variable The next estimator (which is a more complicated case) depends linearly on vector x:[solved!] | Vlavar Engineering" />
<meta name="description" content="The LMMSE estimator is expressed as  hat{theta }=m_{x,theta }^{T}M_{x}^{-1}x. To derive the above result, we model the square form of the"/>
<meta property="og:description" content="The LMMSE estimator is expressed as  hat{theta }=m_{x,theta }^{T}M_{x}^{-1}x. To derive the above result, we model the square form of the" />


<meta name="twitter:description" content="The LMMSE estimator is expressed as  hat{theta }=m_{x,theta }^{T}M_{x}^{-1}x. To derive the above result, we model the square form of the" />
<meta name="twitter:title" content="Optimal Linear Estimator of a Scalar Random Variable The next estimator (which is a more complicated case) depends linearly on vector x:[solved!] | Vlavar Engineering" />

<meta property="article:published_time" content="2022-09-19T14:31:11+04:00" />
        <meta property="og:url" content="https://vlavar.com/engineeringdb/optimal-linear-estimator-of-a-scalar-random-variable-the-next-estimator-which-is-a-more-complicated-case-depends-linearly-on-vector-x" />
        <meta property="article:author" content="admin@vlavar" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:site" content="admin@vlavar" />
        <meta name="twitter:creator" content="admin@vlavar" />
     
    </head>
    
    <body>
        <nav id="desktop" class="desktop">
            <div id="desktop-container" class="container">
                <h1 id="logo"><a href="../index.html">Vlavar</a></h1>
                <div id="desktop-menu" class="menu">
                    <a href="../index.html">Home</a>
                    <a id="search">Search</a>
                    <a href="../tour/1.html">Tour</a>
                    <a href="../contact-us.html">Contact Us</a>
                </div>
                <button id="burger-button" class="hamburger">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
    
            </div>
        </nav>
        <nav id="mobile-navbar" class="mobile-nav">
            <div class="one text-fade"><a class="nav-link" href="../index.html">Home</a></div>
            <div class="two text-fade"><a class="nav-link" id="search-mobile">Search</a></div>
            <div class="three text-fade"><a class="nav-link" href="../tour/1.html">Tour</a></div>
            <div class="four text-fade"><a class="nav-link" href="../contact-us.html">COntact Us</a></div>
        </nav>
    
        <div id="container" class="container-items tour">
            <div id="searchbox" class="searchbox">
                <div class="search">
                    <div class="icon"></div>
                    <form class="form">
                        <div class="input">
                        <input type="text" id="x" placeholder="Find solutions to engineering problems " autocomplete="off" >

                        </div>
                    </form>
                </div>
            </div>
            <div class="items question-block"> <span class="question">Guest Question. </span>
            <p class="blog">Optimal Linear Estimator of a Scalar Random Variable The next estimator (which is a more complicated case) depends linearly on vector x:</p> <p style="text-align: center;"><span class="tex-text">\(\hat{\theta } =h^{T}x,\)</span></p> <p class="blog">where <span class="tex-text">\(h = [h_{1},...,h_{n}]^{T}\)</span> is a set of linear coefficients to be defined. For the implementation of this estimator, the second moment matrix is necessary:</p> <p style="text-align: center;"><span class="tex-text">\(M_{x}=E[xx^{T}],\)</span></p> <p class="blog">as well as the cross-moment vector:</p> <p style="text-align: center;"><span class="tex-text">\(m_{x,\theta }=E[x\theta ].\)</span></p> <p class="blog">Let <span class="tex-text">\(M_{x}\)</span> be the invertible matrix.<br /> The problem is to derive the coefficient vector h that minimizes the MSE:</p> <p style="text-align: center;">MSE(h)=<span class="tex-text">\(E[(\theta -h^{T}x)^{2}].\)</span></p>      
    <!-- end -->
            </div>
    
            <div class="items blog-solution">
                <div class="admin"><span>Admin</span> <svg class="verified-badge" xmlns="http://www.w3.org/2000/svg"
                        version="1.1" viewBox="0,0,24,24">
                        <path
                            d="M22.5 12.5c0-1.58-.875-2.95-2.148-3.6.154-.435.238-.905.238-1.4 0-2.21-1.71-3.998-3.818-3.998-.47 0-.92.084-1.336.25C14.818 2.415 13.51 1.5 12 1.5s-2.816.917-3.437 2.25c-.415-.165-.866-.25-1.336-.25-2.11 0-3.818 1.79-3.818 4 0 .494.083.964.237 1.4-1.272.65-2.147 2.018-2.147 3.6 0 1.495.782 2.798 1.942 3.486-.02.17-.032.34-.032.514 0 2.21 1.708 4 3.818 4 .47 0 .92-.086 1.335-.25.62 1.334 1.926 2.25 3.437 2.25 1.512 0 2.818-.916 3.437-2.25.415.163.865.248 1.336.248 2.11 0 3.818-1.79 3.818-4 0-.174-.012-.344-.033-.513 1.158-.687 1.943-1.99 1.943-3.484zm-6.616-3.334l-4.334 6.5c-.145.217-.382.334-.625.334-.143 0-.288-.04-.416-.126l-.115-.094-2.415-2.415c-.293-.293-.293-.768 0-1.06s.768-.294 1.06 0l1.77 1.767 3.825-5.74c.23-.345.696-.436 1.04-.207.346.23.44.696.21 1.04z"
                            fill="#1da1f2" />
                    </svg> </div>
    
                <div class="solution">Solution</div>
    
            </div>
    
            <div class="items answer-block">
                <p class="blog">
The LMMSE estimator is expressed as <span class="tex-text">\( \hat{\theta }=m_{x,\theta }^{T}M_{x}^{-1}x.\)</span></p>
 <p class="blog">
To derive the above result, we model the square form of the MSE with respect to h:</p>
 <p style="text-align: center">MSE<span class="tex-text">\((\theta )=E[(\theta -\hat{\theta } )^{2}]=E[(\theta -h^{T}x)^{2}]\)</span><br /> <span class="tex-text">\(=h^{T}E[xx^{T}]h+E[\theta ^{2}]-h^{T}E[x\theta ]-E[\theta x^{T}]h.\)</span></p>
 <p class="blog">
The vector h, which minimizes this square function, can be solved by factorization:</p>
 <p style="text-align: center"><span class="tex-text">\(0^{T}=\triangledown _{h}MSE(h)=[\frac{\partial}{\partial h_{1}},&#8230;,\frac{\partial}{\partial h_{n}} ]\)</span>MSE<span class="tex-text">\((\hat{\theta })\)</span><br /> <span class="tex-text">\(=2(h^{T}E[xx^{T}]-E[\theta x^{T}]).\)</span></p>
 <p class="blog">
Therefore, the optimum vector h satisfies the equation:</p>
 <p style="text-align: center"><span class="tex-text">\(E[xx^{T}]h=E[x\theta ]\)</span></p>
 <p class="blog">
Considering the matrix <span class="tex-text">\(M_{x}=E[xx^{T}] \)</span>is non-singular, this is equivalent to:</p>
 <p style="text-align: center"><span class="tex-text">\(h=M_{x}^{-1}m_{x\theta },\)</span></p>
 <p class="blog">
and the optimal linear estimator is derived:</p>
 <p style="text-align: center"><span class="tex-text">\(\hat{\theta } =m_{x\theta }^{T}M_{x}^{-1}x.\)</span></p>
 <p class="blog">
By substituting the above result in the expression of the MSE(h), the minimum MSE for linear estimators is defined as:</p>
 <p style="text-align: center">MSE<span class="tex-text">\(_{min}=E[\theta ^{2}]-m_{x\theta }^{T}M_{x}^{-1}m_{x\theta }.\)</span></p>
 <p class="blog">
It is noteworthy that, as the matrix <span class="tex-text">\(M_{x}^{âˆ’1}\)</span> is positively defined, the MSE cannot exceed the a priori second moment <span class="tex-text">\(E[\theta^{2}]\)</span> of Î¸. If the parameter has a zero mean, then <span class="tex-text">\(E[\theta^{2}] = E[( \thetaâˆ’ E[ \theta])^{2} ]\)</span> = var<span class="tex-text">\(( \theta)\)</span>, i.e., the second moment is equal to the a priori variance, and the LMMSE estimator performance is superior to that of the constant estimator<span class="tex-text">\(\hat{\theta } = E[Î¸ ] = 0.\)</span><br /> However, if E[Î¸ ] â‰  0 then <span class="tex-text">\(E[Î¸^{2} ] &gt; E[(Î¸ âˆ’ E[Î¸ ])^{2} ] \)</span>and the LMMSE estimator may perform worse than the constant estimator. Basically, this problem occurs because the LMMSE estimator is a biased estimator of Î¸, in the sense that its biased mean is <span class="tex-text">\(E[\hat{\theta } ]âˆ’ E[Î¸ ] \)</span>â‰  0, except if E[Î¸ ] = 0. The way to deal with this bias is by generalizing the category of linear estimators to that of the affine estimators.</p>
 
    
          <!-- end -->
            </div>
        </div>
    
    
        <footer class="footer-bg footer-slct">
            <div class="footer-top">
                <a href="../tos.html">terms</a>
                <a href="../privacy.html">privacy</a>
                <a href="../contact-us.html">contact us</a>
            </div>
            <div class="footer-bottom">
                &copy; 2022 Vlavar Inc. All Rights Reserved.
            </div>
        </footer>    
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js" integrity="sha384-X/XCfMm41VSsqRNQgDerQczD69XqmjOOOwYQvr/uuC+j4OPoNhVgjdGFwhvN02Ja" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script>
        <script src="../assets/js/min/client-ui.js"></script>
    </body>
    
    </html>